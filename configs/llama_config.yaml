# Model configuration
model_name: "meta-llama/Llama-3.2-1B"
num_labels: 2
max_length: 512

# Training configuration
batch_size: 64
learning_rate: 0.0005
num_epochs: 20 #3
gradient_accumulation_steps: 4
warmup_steps: 100
weight_decay: 0.01
num_workers: 16

# LoRA configuration
lora_r: 8
lora_alpha: 32
lora_dropout: 0.1

# Uncertainty configuration
seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
num_ensemble_models: 10

# Paths
output_dir: "model_outputs"
cache_dir: "cache"

# Hardware configuration
use_bf16: true
num_gpus: 1
